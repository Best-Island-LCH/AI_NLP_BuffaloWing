{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RLHF Evaluation Notebook\n",
        "\n",
        "This notebook evaluates a base model vs an RLHF model using:\n",
        "1) RM-based scoring\n",
        "2) GPT-based preference judging\n",
        "3) Style/structure metrics\n",
        "\n",
        "Outputs: a markdown report and raw CSV/JSON artifacts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (Colab)\n",
        "!pip -q install transformers accelerate sentencepiece openai tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from glob import glob\n",
        "from collections import Counter, defaultdict\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# ---- Config ----\n",
        "DATA_ROOT = \"/content/drive/Mydrive/Likelion/실전 프로젝트 2/train\"\n",
        "DATA_GLOB = \"**/RMlabel.json\"\n",
        "MAX_SAMPLES = 50  # adjust\n",
        "SEED = 42\n",
        "\n",
        "BASE_MODEL_ID = \"your-base-model-id\"\n",
        "RLHF_MODEL_ID = \"your-rlhf-model-id\"\n",
        "REWARD_MODEL_ID = \"your-reward-model-id\"\n",
        "\n",
        "GEN_KWARGS = dict(max_new_tokens=512, temperature=0.7, top_p=0.9, do_sample=True)\n",
        "\n",
        "RUN_GPT_EVAL = False  # set True if you have API access\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "TIMESTAMP = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "REPORT_PATH = f\"/content/drive/Mydrive/Likelion/실전 프로젝트 2/evaluation/rlhf_eval_report_{TIMESTAMP}.md\"\n",
        "ARTIFACT_DIR = f\"/content/drive/Mydrive/Likelion/실전 프로젝트 2/evaluation/artifacts_{TIMESTAMP}\"\n",
        "GEN_DIR = os.path.join(ARTIFACT_DIR, \"generations\")\n",
        "EVAL_DIR = os.path.join(ARTIFACT_DIR, \"eval\")\n",
        "os.makedirs(GEN_DIR, exist_ok=True)\n",
        "os.makedirs(EVAL_DIR, exist_ok=True)\n",
        "\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_rm_label(path):\n",
        "    # Try utf-8 first, then cp949 to recover Korean in legacy files\n",
        "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp949\"):\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=enc) as f:\n",
        "                data = json.load(f)\n",
        "            break\n",
        "        except Exception:\n",
        "            data = None\n",
        "    if data is None:\n",
        "        raise ValueError(f\"Failed to decode {path}\")\n",
        "    items = data.get(\"data_info\", [])\n",
        "    rows = []\n",
        "    for item in items:\n",
        "        q = item.get(\"question\", \"\").strip()\n",
        "        if not q:\n",
        "            continue\n",
        "        rows.append({\"data_id\": item.get(\"data_id\"), \"question\": q})\n",
        "    return rows\n",
        "\n",
        "def collect_questions(root, pattern):\n",
        "    paths = sorted(glob(os.path.join(root, pattern), recursive=True))\n",
        "    rows = []\n",
        "    for p in paths:\n",
        "        rows.extend(load_rm_label(p))\n",
        "    return rows, paths\n",
        "\n",
        "rows, paths = collect_questions(DATA_ROOT, DATA_GLOB)\n",
        "print(f\"Loaded {len(rows)} questions from {len(paths)} files\")\n",
        "df = pd.DataFrame(rows).drop_duplicates(subset=[\"data_id\"])\n",
        "if MAX_SAMPLES:\n",
        "    df = df.sample(min(MAX_SAMPLES, len(df)), random_state=SEED)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_causal_model(model_id):\n",
        "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "    return model, tok\n",
        "\n",
        "def load_reward_model(model_id):\n",
        "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "    return model, tok\n",
        "\n",
        "base_model, base_tok = load_causal_model(BASE_MODEL_ID)\n",
        "rlhf_model, rlhf_tok = load_causal_model(RLHF_MODEL_ID)\n",
        "rm_model, rm_tok = load_reward_model(REWARD_MODEL_ID)\n",
        "\n",
        "base_model.eval()\n",
        "rlhf_model.eval()\n",
        "rm_model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_prompt(question, tokenizer):\n",
        "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ]\n",
        "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return f\"Question:\\n{question}\\n\\nAnswer:\\n\"\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_answer(model, tokenizer, question, **gen_kwargs):\n",
        "    prompt = build_prompt(question, tokenizer)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, **gen_kwargs)\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # strip prompt if present\n",
        "    if text.startswith(prompt):\n",
        "        text = text[len(prompt):]\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rm_format(question, answer):\n",
        "    return f\"Human: {question}\\nAssistant: {answer}\"\n",
        "\n",
        "@torch.inference_mode()\n",
        "def rm_score(question, answer, model, tokenizer):\n",
        "    text = rm_format(question, answer)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "    out = model(**inputs)\n",
        "    # assume single logit\n",
        "    score = out.logits.squeeze().float().item()\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GPT_PROMPT = '''You are an evaluator judging which answer better matches human preference.\n",
        "\n",
        "Question:\n",
        "{Q}\n",
        "\n",
        "Answer A:\n",
        "{A}\n",
        "\n",
        "Answer B:\n",
        "{B}\n",
        "\n",
        "Evaluation criteria:\n",
        "- Clear and well-structured explanation\n",
        "- Formal written style\n",
        "- Minimal redundancy\n",
        "- Appropriate professional and safe tone\n",
        "- Directly addresses the question\n",
        "\n",
        "Choose which answer is better overall.\n",
        "Output strictly in the following format:\n",
        "Winner: A\n",
        "Reasons:\n",
        "- Clear and well-structured explanation : A\n",
        "- Formal written style : B\n",
        "- Minimal redundancy : B\n",
        "- Appropriate professional and safe tone : A\n",
        "- Directly addresses the question : A\n",
        "Reason: one short sentence\n",
        "'''\n",
        "\n",
        "def parse_gpt_judge(text):\n",
        "    winner = None\n",
        "    m = re.search(r\"Winner:\\s*([AB])\", text)\n",
        "    if m:\n",
        "        winner = m.group(1)\n",
        "    return winner, text\n",
        "\n",
        "def gpt_judge(question, ans_a, ans_b, client, model=OPENAI_MODEL):\n",
        "    prompt = GPT_PROMPT.format(Q=question, A=ans_a, B=ans_b)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.0,\n",
        "    )\n",
        "    text = resp.choices[0].message.content\n",
        "    return parse_gpt_judge(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c3cb817",
      "metadata": {},
      "outputs": [],
      "source": [
        "KOR_STOPWORDS = set([\n",
        "    \"이\", \"그\", \"저\", \"것\", \"수\", \"등\", \"및\", \"에서\", \"으로\", \"에게\", \"으로써\",\n",
        "    \"하다\", \"되다\", \"있다\", \"없다\", \"그리고\", \"하지만\", \"또한\", \"그러나\", \"때문\", \"때문에\", \"및\", \"대한\", \"관련\"\n",
        "])\n",
        "EN_STOPWORDS = set([\"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"to\", \"of\", \"in\", \"for\", \"on\", \"with\"])\n",
        "\n",
        "FORMAL_ENDINGS = [\"입니다\", \"합니다\", \"됩니다\", \"하십시오\", \"습니다\"]\n",
        "COLLOQUIAL_ENDINGS = [\"요\", \"죠\", \"음\"]\n",
        "IMPERATIVE_ENDINGS = [\"하세요\", \"하십시오\", \"하라\", \"하세요\"]\n",
        "SPECULATIVE = [\"일 수\", \"가능\", \"추정\", \"보입니다\", \"같습니다\"]\n",
        "APOLOGY = [\"죄송\", \"유감\"]\n",
        "HEDGE = [\"약간\", \"어느 정도\", \"아마\", \"대체로\"]\n",
        "DEFENSIVE = [\"도움이 되길 바랍니다\", \"참고 바랍니다\"]\n",
        "ASSERTIVE = [\"반드시\", \"항상\", \"절대\", \"무조건\"]\n",
        "RISKY = [\"자살\", \"살해\", \"폭탄\", \"불법\", \"마약\"]\n",
        "CONDITIONAL = [\"만약\", \"경우\", \"때\", \"라면\"]\n",
        "NEUTRAL = [\"일반적으로\", \"대개\", \"보통\", \"중립\", \"권장\"]\n",
        "CONJ = [\"그리고\", \"하지만\", \"또한\", \"그러나\", \"즉\", \"따라서\"]\n",
        "SUBORD = [\"때문에\", \"면서\", \"도록\", \"므로\", \"는데\"]\n",
        "\n",
        "def tokenize(text):\n",
        "    # simple mixed tokenizer (Korean/English/digits)\n",
        "    return re.findall(r\"[A-Za-z]+|[0-9]+|[가-힣]+\", text)\n",
        "\n",
        "def split_sentences(text):\n",
        "    parts = re.split(r\"(?<=[\\.\\!\\?])\\s+|\\n+\", text.strip())\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "def split_paragraphs(text):\n",
        "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
        "    return paras if paras else [text.strip()]\n",
        "\n",
        "def ngram_repetition(tokens, n):\n",
        "    if len(tokens) < n:\n",
        "        return 0.0\n",
        "    ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "    counts = Counter(ngrams)\n",
        "    repeated = sum(c for c in counts.values() if c > 1)\n",
        "    return repeated / max(1, len(ngrams))\n",
        "\n",
        "def cosine_sim(a, b):\n",
        "    common = set(a) | set(b)\n",
        "    if not common:\n",
        "        return 0.0\n",
        "    va = np.array([a.get(k, 0) for k in common])\n",
        "    vb = np.array([b.get(k, 0) for k in common])\n",
        "    denom = (np.linalg.norm(va) * np.linalg.norm(vb))\n",
        "    if denom == 0:\n",
        "        return 0.0\n",
        "    return float(np.dot(va, vb) / denom)\n",
        "\n",
        "def sentence_similarity_avg(sentences):\n",
        "    if len(sentences) < 2:\n",
        "        return 0.0\n",
        "    vecs = []\n",
        "    for s in sentences:\n",
        "        toks = tokenize(s)\n",
        "        vecs.append(Counter(toks))\n",
        "    sims = []\n",
        "    for i in range(len(vecs)):\n",
        "        for j in range(i+1, len(vecs)):\n",
        "            sims.append(cosine_sim(vecs[i], vecs[j]))\n",
        "    return float(np.mean(sims)) if sims else 0.0\n",
        "\n",
        "def keyword_overuse_ratio(tokens):\n",
        "    if not tokens:\n",
        "        return 0.0\n",
        "    counts = Counter(tokens)\n",
        "    top = sum(c for _, c in counts.most_common(5))\n",
        "    return top / len(tokens)\n",
        "\n",
        "def list_usage_ratio(text):\n",
        "    lines = [l for l in text.splitlines() if l.strip()]\n",
        "    if not lines:\n",
        "        return 0.0\n",
        "    list_lines = sum(1 for l in lines if re.match(r\"\\s*(\\d+\\.|-|\\*)\\s+\", l))\n",
        "    return list_lines / len(lines)\n",
        "\n",
        "def intro_body_conclusion(text):\n",
        "    paras = split_paragraphs(text)\n",
        "    if len(paras) < 2:\n",
        "        return 0\n",
        "    intro = bool(re.search(r\"서론|개요|먼저\", paras[0]))\n",
        "    concl = bool(re.search(r\"결론|요약|마무리\", paras[-1]))\n",
        "    return int(intro and concl)\n",
        "\n",
        "def last_sentence_summary(sentences):\n",
        "    if not sentences:\n",
        "        return 0\n",
        "    return int(bool(re.search(r\"요약|정리|결론\", sentences[-1])))\n",
        "\n",
        "def ending_ratio(text, endings):\n",
        "    sentences = split_sentences(text)\n",
        "    if not sentences:\n",
        "        return 0.0\n",
        "    hits = 0\n",
        "    for s in sentences:\n",
        "        if any(s.endswith(e) for e in endings):\n",
        "            hits += 1\n",
        "    return hits / len(sentences)\n",
        "\n",
        "def count_ratio(text, patterns):\n",
        "    if not text:\n",
        "        return 0.0\n",
        "    count = sum(text.count(p) for p in patterns)\n",
        "    return count / max(1, len(split_sentences(text)))\n",
        "\n",
        "def comma_density(text):\n",
        "    sentences = split_sentences(text)\n",
        "    if not sentences:\n",
        "        return 0.0\n",
        "    commas = sum(s.count(\",\") for s in sentences)\n",
        "    return commas / len(sentences)\n",
        "\n",
        "def conjunction_density(text):\n",
        "    sentences = split_sentences(text)\n",
        "    if not sentences:\n",
        "        return 0.0\n",
        "    hits = sum(1 for s in sentences if any(c in s for c in CONJ))\n",
        "    return hits / len(sentences)\n",
        "\n",
        "def fragment_ratio(text):\n",
        "    sentences = split_sentences(text)\n",
        "    if not sentences:\n",
        "        return 0.0\n",
        "    endings = FORMAL_ENDINGS + COLLOQUIAL_ENDINGS\n",
        "    fragments = sum(1 for s in sentences if not any(s.endswith(e) for e in endings))\n",
        "    return fragments / len(sentences)\n",
        "\n",
        "def entity_clarity_ratio(text):\n",
        "    sentences = split_sentences(text)\n",
        "    if not sentences:\n",
        "        return 0.0\n",
        "    hits = sum(1 for s in sentences if re.search(r\"(은|는|이|가)\", s))\n",
        "    return hits / len(sentences)\n",
        "\n",
        "def keyword_coverage(question, answer):\n",
        "    q_tokens = [t for t in tokenize(question) if t not in KOR_STOPWORDS and t not in EN_STOPWORDS]\n",
        "    if not q_tokens:\n",
        "        return 0.0\n",
        "    a_tokens = set(tokenize(answer))\n",
        "    covered = sum(1 for t in set(q_tokens) if t in a_tokens)\n",
        "    return covered / len(set(q_tokens))\n",
        "\n",
        "def proper_noun_ratio(tokens):\n",
        "    if not tokens:\n",
        "        return 0.0\n",
        "    # heuristic: English capitalized tokens or tokens with digits\n",
        "    hits = sum(1 for t in tokens if re.search(r\"[A-Z]\", t) or re.search(r\"\\d\", t))\n",
        "    return hits / len(tokens)\n",
        "\n",
        "def info_units_per_sentence(text):\n",
        "    sentences = split_sentences(text)\n",
        "    if not sentences:\n",
        "        return 0.0\n",
        "    units = []\n",
        "    for s in sentences:\n",
        "        toks = [t for t in tokenize(s) if t not in KOR_STOPWORDS and t not in EN_STOPWORDS]\n",
        "        units.append(len(set(toks)))\n",
        "    return float(np.mean(units))\n",
        "\n",
        "def stopword_ratio(tokens):\n",
        "    if not tokens:\n",
        "        return 0.0\n",
        "    hits = sum(1 for t in tokens if t in KOR_STOPWORDS or t in EN_STOPWORDS)\n",
        "    return hits / len(tokens)\n",
        "\n",
        "def redundant_sentence_ratio(sentences, threshold=0.8):\n",
        "    if len(sentences) < 2:\n",
        "        return 0.0\n",
        "    vecs = [Counter(tokenize(s)) for s in sentences]\n",
        "    redundant = 0\n",
        "    total = 0\n",
        "    for i in range(len(vecs)):\n",
        "        for j in range(i+1, len(vecs)):\n",
        "            total += 1\n",
        "            if cosine_sim(vecs[i], vecs[j]) >= threshold:\n",
        "                redundant += 1\n",
        "    return redundant / total if total else 0.0\n",
        "\n",
        "def viewpoint_changes(text):\n",
        "    pronouns = [\"저\", \"나\", \"우리\", \"당신\", \"그\", \"그녀\", \"그들\"]\n",
        "    tokens = tokenize(text)\n",
        "    seq = [t for t in tokens if t in pronouns]\n",
        "    if len(seq) < 2:\n",
        "        return 0\n",
        "    changes = sum(1 for i in range(1, len(seq)) if seq[i] != seq[i-1])\n",
        "    return changes\n",
        "\n",
        "def order_stability(text):\n",
        "    nums = [int(n) for n in re.findall(r\"\\b(\\d+)\\.\", text)]\n",
        "    if len(nums) < 2:\n",
        "        return 1.0\n",
        "    stable = all(nums[i] <= nums[i+1] for i in range(len(nums)-1))\n",
        "    return 1.0 if stable else 0.0\n",
        "\n",
        "def style_features(question, answer):\n",
        "    tokens = tokenize(answer)\n",
        "    sentences = split_sentences(answer)\n",
        "    paras = split_paragraphs(answer)\n",
        "\n",
        "    feats = {}\n",
        "    # 1) length/structure\n",
        "    feats[\"token_count\"] = len(tokens)\n",
        "    feats[\"paragraph_count\"] = len(paras)\n",
        "    feats[\"avg_sentences_per_paragraph\"] = float(np.mean([len(split_sentences(p)) for p in paras])) if paras else 0.0\n",
        "    feats[\"list_usage_ratio\"] = list_usage_ratio(answer)\n",
        "    feats[\"intro_body_conclusion\"] = intro_body_conclusion(answer)\n",
        "    feats[\"first_sentence_length\"] = len(tokenize(sentences[0])) if sentences else 0\n",
        "    feats[\"last_sentence_summary\"] = last_sentence_summary(sentences)\n",
        "\n",
        "    # 2) repetition\n",
        "    feats[\"ngram_rep_2\"] = ngram_repetition(tokens, 2)\n",
        "    feats[\"ngram_rep_3\"] = ngram_repetition(tokens, 3)\n",
        "    feats[\"ngram_rep_4\"] = ngram_repetition(tokens, 4)\n",
        "    feats[\"sentence_similarity_avg\"] = sentence_similarity_avg(sentences)\n",
        "    feats[\"phrase_repeat_rate\"] = answer.count(\"다음과 같습니다\") / max(1, len(sentences))\n",
        "    feats[\"keyword_overuse_ratio\"] = keyword_overuse_ratio(tokens)\n",
        "\n",
        "    # 3) style/tone\n",
        "    feats[\"formal_ending_ratio\"] = ending_ratio(answer, FORMAL_ENDINGS)\n",
        "    feats[\"colloquial_ratio\"] = ending_ratio(answer, COLLOQUIAL_ENDINGS)\n",
        "    feats[\"exclaim_ratio\"] = answer.count(\"!\") / max(1, len(sentences))\n",
        "    feats[\"question_mark_ratio\"] = answer.count(\"?\") / max(1, len(sentences))\n",
        "    feats[\"first_person_ratio\"] = count_ratio(answer, [\"저는\", \"제가\", \"나는\", \"우리\"])\n",
        "    feats[\"imperative_ratio\"] = count_ratio(answer, IMPERATIVE_ENDINGS)\n",
        "    feats[\"speculative_ratio\"] = count_ratio(answer, SPECULATIVE)\n",
        "\n",
        "    # 4) apology/hedge/defensive\n",
        "    feats[\"apology_ratio\"] = count_ratio(answer, APOLOGY)\n",
        "    feats[\"defensive_ratio\"] = count_ratio(answer, DEFENSIVE)\n",
        "    feats[\"hedge_ratio\"] = count_ratio(answer, HEDGE)\n",
        "\n",
        "    # 5) clarity\n",
        "    feats[\"avg_sentence_length\"] = float(np.mean([len(tokenize(s)) for s in sentences])) if sentences else 0.0\n",
        "    feats[\"subordinate_ratio\"] = count_ratio(answer, SUBORD)\n",
        "    feats[\"comma_density\"] = comma_density(answer)\n",
        "    feats[\"conjunction_density\"] = conjunction_density(answer)\n",
        "    feats[\"fragment_ratio\"] = fragment_ratio(answer)\n",
        "    feats[\"entity_clarity_ratio\"] = entity_clarity_ratio(answer)\n",
        "\n",
        "    # 6) info density\n",
        "    feats[\"proper_noun_ratio\"] = proper_noun_ratio(tokens)\n",
        "    feats[\"keyword_coverage\"] = keyword_coverage(question, answer)\n",
        "    feats[\"info_units_per_sentence\"] = info_units_per_sentence(answer)\n",
        "    feats[\"stopword_ratio\"] = stopword_ratio(tokens)\n",
        "    feats[\"redundant_sentence_ratio\"] = redundant_sentence_ratio(sentences)\n",
        "\n",
        "    # 7) safety/neutral\n",
        "    feats[\"assertive_ratio\"] = count_ratio(answer, ASSERTIVE)\n",
        "    feats[\"risky_keyword_ratio\"] = count_ratio(answer, RISKY)\n",
        "    feats[\"conditional_ratio\"] = count_ratio(answer, CONDITIONAL)\n",
        "    feats[\"neutral_vocab_ratio\"] = count_ratio(answer, NEUTRAL)\n",
        "\n",
        "    # 8) consistency\n",
        "    feats[\"question_keyword_reuse\"] = keyword_coverage(question, answer)\n",
        "    feats[\"viewpoint_changes\"] = viewpoint_changes(answer)\n",
        "    feats[\"logic_connector_consistency\"] = conjunction_density(answer)\n",
        "    feats[\"order_stability\"] = order_stability(answer)\n",
        "\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stage 1: generate and save JSONL per model\n",
        "import json\n",
        "\n",
        "base_out = os.path.join(GEN_DIR, \"base_generations.jsonl\")\n",
        "rlhf_out = os.path.join(GEN_DIR, \"rlhf_generations.jsonl\")\n",
        "\n",
        "def write_jsonl(path, rows):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "base_rows = []\n",
        "rlhf_rows = []\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    q = row[\"question\"]\n",
        "    data_id = row[\"data_id\"]\n",
        "    base_ans = generate_answer(base_model, base_tok, q, **GEN_KWARGS)\n",
        "    rlhf_ans = generate_answer(rlhf_model, rlhf_tok, q, **GEN_KWARGS)\n",
        "\n",
        "    base_rows.append({\"data_id\": data_id, \"question\": q, \"answer\": base_ans})\n",
        "    rlhf_rows.append({\"data_id\": data_id, \"question\": q, \"answer\": rlhf_ans})\n",
        "\n",
        "write_jsonl(base_out, base_rows)\n",
        "write_jsonl(rlhf_out, rlhf_rows)\n",
        "print(base_out)\n",
        "print(rlhf_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stage 2: evaluate from saved JSONL\n",
        "import json\n",
        "\n",
        "def read_jsonl(path):\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "base_rows = read_jsonl(base_out)\n",
        "rlhf_rows = read_jsonl(rlhf_out)\n",
        "base_map = {r[\"data_id\"]: r for r in base_rows}\n",
        "rlhf_map = {r[\"data_id\"]: r for r in rlhf_rows}\n",
        "\n",
        "results = []\n",
        "if RUN_GPT_EVAL:\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI()\n",
        "else:\n",
        "    client = None\n",
        "\n",
        "for data_id in tqdm(sorted(set(base_map) & set(rlhf_map))):\n",
        "    q = base_map[data_id][\"question\"]\n",
        "    base_ans = base_map[data_id][\"answer\"]\n",
        "    rlhf_ans = rlhf_map[data_id][\"answer\"]\n",
        "\n",
        "    base_rm = rm_score(q, base_ans, rm_model, rm_tok)\n",
        "    rlhf_rm = rm_score(q, rlhf_ans, rm_model, rm_tok)\n",
        "\n",
        "    gpt_winner = None\n",
        "    gpt_raw = None\n",
        "    if RUN_GPT_EVAL:\n",
        "        gpt_winner, gpt_raw = gpt_judge(q, rlhf_ans, base_ans, client)\n",
        "\n",
        "    base_feats = style_features(q, base_ans)\n",
        "    rlhf_feats = style_features(q, rlhf_ans)\n",
        "\n",
        "    results.append({\n",
        "        \"data_id\": data_id,\n",
        "        \"question\": q,\n",
        "        \"base_answer\": base_ans,\n",
        "        \"rlhf_answer\": rlhf_ans,\n",
        "        \"base_rm_score\": base_rm,\n",
        "        \"rlhf_rm_score\": rlhf_rm,\n",
        "        \"gpt_winner\": gpt_winner,\n",
        "        \"gpt_raw\": gpt_raw,\n",
        "        \"base_features\": base_feats,\n",
        "        \"rlhf_features\": rlhf_feats,\n",
        "    })\n",
        "\n",
        "len(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flatten results for analysis\n",
        "rows = []\n",
        "for r in results:\n",
        "    base = {\"model\": \"base\", \"data_id\": r[\"data_id\"], \"rm_score\": r[\"base_rm_score\"], **r[\"base_features\"]}\n",
        "    rlhf = {\"model\": \"rlhf\", \"data_id\": r[\"data_id\"], \"rm_score\": r[\"rlhf_rm_score\"], **r[\"rlhf_features\"]}\n",
        "    rows.extend([base, rlhf])\n",
        "\n",
        "feat_df = pd.DataFrame(rows)\n",
        "feat_df.to_csv(os.path.join(EVAL_DIR, \"style_metrics.csv\"), index=False)\n",
        "\n",
        "raw_path = os.path.join(EVAL_DIR, \"raw_results.json\")\n",
        "with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# GPT win rate\n",
        "gpt_wins = [r[\"gpt_winner\"] for r in results if r[\"gpt_winner\"]]\n",
        "gpt_win_rate = None\n",
        "if gpt_wins:\n",
        "    gpt_win_rate = gpt_wins.count(\"A\") / len(gpt_wins)\n",
        "\n",
        "summary = feat_df.groupby(\"model\").agg(['mean', 'std'])\n",
        "summary.to_csv(os.path.join(EVAL_DIR, \"style_metrics_summary.csv\"))\n",
        "summary.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_report(path, feat_df, gpt_win_rate, dataset_paths):\n",
        "    models = feat_df[\"model\"].unique().tolist()\n",
        "    metrics = [c for c in feat_df.columns if c not in [\"model\", \"data_id\"]]\n",
        "\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"# RLHF Evaluation Report\\n\\n\")\n",
        "        f.write(\"## Dataset\\n\")\n",
        "        f.write(f\"- Source files: {len(dataset_paths)}\\n\")\n",
        "        f.write(f\"- Samples used: {feat_df[\"data_id\"].nunique()}\\n\\n\")\n",
        "\n",
        "        f.write(\"## RM Score\\n\")\n",
        "        for m in models:\n",
        "            mean = feat_df[feat_df[\"model\"] == m][\"rm_score\"].mean()\n",
        "            std = feat_df[feat_df[\"model\"] == m][\"rm_score\"].std()\n",
        "            f.write(f\"- {m}: mean={mean:.4f}, std={std:.4f}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"## GPT Preference (A=RLHF, B=Base)\\n\")\n",
        "        if gpt_win_rate is None:\n",
        "            f.write(\"- GPT eval not run.\\n\")\n",
        "        else:\n",
        "            f.write(f\"- RLHF win rate: {gpt_win_rate:.3f}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"## Style Metrics (mean ? std)\\n\")\n",
        "        for metric in metrics:\n",
        "            if metric == \"rm_score\":\n",
        "                continue\n",
        "            f.write(f\"### {metric}\\n\")\n",
        "            for m in models:\n",
        "                s = feat_df[feat_df[\"model\"] == m][metric]\n",
        "                f.write(f\"- {m}: {s.mean():.4f} ? {s.std():.4f}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"## Notes\\n\")\n",
        "        f.write(\"- Style metrics use lightweight heuristics. Replace with a richer tokenizer or domain-specific analyzer if needed.\\n\")\n",
        "\n",
        "write_report(REPORT_PATH, feat_df, gpt_win_rate, paths)\n",
        "REPORT_PATH\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
