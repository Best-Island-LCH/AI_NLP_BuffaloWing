# 📊 [Report] LLM 베이스라인 모델 성능 비교 분석 (3차)

## 1. 실험 개요 (Experiment Overview)
본 실험은 10,580건의 SFT 데이터를 학습시키기 전, 각 모델의 기초 지능과 한국어 지시 이행 능력을 평가하는 데 목적이 있습니다.

**실험 환경**: Google Colab A100 GPU (VRAM 40GB)

**평가 데이터**: NIA097 SFT 데이터셋 기반 무작위 10개 문항

**평가 지표**: ROUGE-1, ROUGE-L (문장 구조 및 키워드 일치도)

## 2. 정량적 성능 지표(Quantitative Results)
| **모델명** | **ROUGH-1** | **ROUGH-L** | **AVG_Length** | **종합 평가** |
| --- | --- | --- | --- | --- |
| EXAONE (1.2B) | 0.2494 | 0.2494 | 266.81 | 최종 선정 (최고 효율)
| kanana-nano (2.1B) | 0.2299 | 0.2299 | 240.43 | 우수 (안정적 추론) | 
| KANANA-1.5 (8B) | 0.1446 | 0.1446 | 230.00 | 보통 (체급 대비 지표 낮음) 
| SOLAR (10.7B) | 0.1033 | 0.0964 | 286.30 | 미흡 (환각 및 언어 혼동) |

## 📈 ROUGE 지표 해석
ROUGE-L 점수는 최장 공통 부분 수열(LCS)을 기반으로 문장의 논리적 흐름을 평가합니다. EXAONE이 기록한 0.2494는 모델이 한국어 지시 사항의 구조를 가장 정확하게 파악하고 있음을 의미합니다.

$$ROUGE\text{-}L = \frac{(1 + \beta^2) R_{lcs} P_{lcs}}{R_{lcs} + \beta^2 P_{lcs}}$$

## 3. 모델별 정성적 분석 (Qualitative Insights)

**✅ EXAONE 4.0 1.2B**: 논리적 구조화의 강점 
* **가독성**: 질문 1(자동화 시스템)과 질문 2(토목공사)에서 볼 수 있듯이, Markdown 헤더(###)를 사용하여 답변을 계층적으로 구조화하는 능력이 매우 뛰어납니다.

* **전문 용어**: 신소재 공학이나 기독교 예배의 형태 등 전문적인 주제에서 정확한 학술적 용어를 구사합니다.

**✅ KANANA-Nano 2.1B**: 자연스러운 한국어 구사

* **지시 이행**: 질문 4(개의 금기 음식) 등 리스트 형태의 답변이 필요한 문항에서 정답(Reference)과 매우 유사한 흐름을 유지합니다.

* **범용성**: 일상적인 추천(미국 야경 도시)부터 기술적 질문까지 답변의 품질이 매우 고르게 분포되어 있습니다.

**✅ KANANA-1.5 8B**: 풍부한 지식량과 깊이 있는 해석

* **지식의 밀도**: 8B 체급답게 질문 2(토목공사)에서 한자어의 어원('토'와 '목')을 직접 풀이하거나, 질문 3(종교 신화)에서 교리의 상징성을 철학적으로 분석하는 등 정보의 깊이가 상당합니다.

* **제로샷 특성**: 답변의 정보량은 매우 풍부하나, 정량적 수치(ROUGE-L 0.14)에서 보듯 제로샷 환경에서는 정답지의 요약된 형식을 따르기보다 상세한 설명을 덧붙이는 경향이 있어 지시 이행 정밀도는 소형 모델 대비 낮게 측정되었습니다.

**❌ SOLAR-10.7B**: 심각한 환각 및 언어 고정 실패

**데이터 오염**: 질문 1에서 특정 기업(3M)의 통계 수치를 반복 출력하거나, 질문 8에서 뜬금없이 기상 관측 데이터를 출력하는 등 사전 학습 데이터의 간섭(Hallucination)이 심각합니다.

**언어 전환**: 한국어 질문임에도 불구하고 답변을 영어로 생성하거나 질문 자체를 영어로 재번역하는 등 언어 일관성이 부족합니다.

## 4. 결론(Conclusion)
**[1순위]**
**✅ EXAONE 4.0 (1.2B)**
**[2순위]**
**✅ KANANA-1.5 (8B)**