import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser   

from peft import PeftModel

class Pipeline:
    def __init__(
        self,
        model_name: str = "kakaocorp/kanana-1.5-8b-instruct-2505",
        adapter_name: str = "jinn33/kanana-1.5-8b-rlhf",
        max_new_tokens: int = 128,
        do_sample: bool = False,
    ):
        # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # 2. Base ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ“¦ Base ëª¨ë¸ ë¡œë“œ: {self.model_name}")
        base_model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        
        # 3. PEFT ì–´ëŒ‘í„° ë¡œë“œ ë° ë³‘í•©
        print(f"ğŸ”§ RLHF ì–´ëŒ‘í„° ë¡œë“œ: {adapter_name}")
        self.model = PeftModel.from_pretrained(
            base_model,
            adapter_name,
            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
        ).eval()
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        #2.í—ˆê¹…í˜ì´ìŠ¤ ë­ì²´ì¸ íŒŒì´í”„ë¼ì¸ 
        self.gen_pipe= pipeline(
             task="text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            return_full_text=True,  
        )   

        self.hf_llm = HuggingFacePipeline(pipeline=self.gen_pipe)

        #3.í”„ë¡¬í”„íŠ¸ 
        self.answer_style_guide ="""
[ANSWER STYLE]
- ë‹µë³€ì€ ê°€ëŠ¥í•˜ë©´ "ë„ì… 1ë¬¸ì¥ + ë²ˆí˜¸ ëª©ë¡(3~7ê°œ) + ë§ˆë¬´ë¦¬ 1ë¬¸ì¥" êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤.
- ë²ˆí˜¸ ëª©ë¡ ê° í•­ëª©ì€ "í‚¤ì›Œë“œ: 1~2ë¬¸ì¥ ì„¤ëª…" í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•œë‹¤.
- í†¤ì€ ì¤‘ë¦½ì ìœ¼ë¡œ "~í•  ìˆ˜ ìˆë‹¤/ê³ ë ¤í•´ì•¼ í•œë‹¤"ë¥¼ ì‚¬ìš©í•œë‹¤.
- ë‹µë³€ì€ ìµœëŒ€ 10ì¤„(ë˜ëŠ” 800ì) ì´ë‚´ë¡œ ì‘ì„±í•œë‹¤.
- ë‹¨, ì§ˆë¬¸ì´ ìˆ«ì/ì—°ë„/ì¸ë¬¼/ê³„ì‚° ë“± ë‹¨ë‹µí˜•ì´ë©´ ëª©ë¡ ì—†ì´ í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µí•œë‹¤.
- ë§íˆ¬ëŠ” ì˜ˆì˜ ìˆê³  ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•œë‹¤.
""".strip()
        self.system_policy=f"""
ë„ˆëŠ” ì¼ë°˜ ì§€ì‹ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì±—ë´‡ì´ë‹¤.
ì•„ë˜ ê·œì¹™ê³¼ ìŠ¤íƒ€ì¼ì„ ë°˜ë“œì‹œ ë”°ë¥¸ë‹¤.    

[SAFETY / UNCERTAINTY]
- í™•ì‹¤í•œ ê·¼ê±°ê°€ ì—†ìœ¼ë©´ "Insufficient information"ì´ë¼ê³ ë§Œ ë‹µí•œë‹¤. (ì¶”ê°€ ì„¤ëª… ê¸ˆì§€)
- ì˜ˆì¸¡/ë¯¸ë˜/í™•ë¥ ì  ì§ˆë¬¸(â€œ~í• ê¹Œìš”?â€)ì—ì„œ í™•ì • ê·¼ê±°ê°€ ì—†ìœ¼ë©´ "Insufficient information"ì„ ìš°ì„ í•œë‹¤.
- ê·¼ê±°ê°€ ë¶ˆì¶©ë¶„í•œ ì¼ë°˜ë¡ ë§Œ ë§í•˜ê²Œ ë  ê²½ìš°, ë‹µë³€ì€ "Insufficient information"ì„ ìš°ì„ í•œë‹¤.

{self.answer_style_guide}
""".strip()  


        self.fewshot_examples="""
[EXAMPLES]
User Question: ìë™ì°¨ ë°°ê¸°ê°€ìŠ¤ì˜ ìœ„í—˜ì€ ë¬´ì—‡ì¸ê°€?
Assistant:
ìë™ì°¨ ë°°ê¸°ê°€ìŠ¤ëŠ” í™˜ê²½ê³¼ ê±´ê°•ì— ì—¬ëŸ¬ ë¶€ì •ì  ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
1. ëŒ€ê¸° ì˜¤ì—¼: ë¯¸ì„¸ë¨¼ì§€Â·ì§ˆì†Œì‚°í™”ë¬¼ ë“±ìœ¼ë¡œ ê³µê¸° ì§ˆì´ ë‚˜ë¹ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. ê±´ê°• ì˜í–¥: í˜¸í¡ê¸°Â·ëˆˆÂ·ì½” ìê·¹ì„ ìœ ë°œí•  ìˆ˜ ìˆê³ , ì¥ê¸° ë…¸ì¶œ ì‹œ ìœ„í—˜ì´ ì»¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. ê¸°í›„ ë³€í™”: ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œë¡œ ì§€êµ¬ ì˜¨ë‚œí™”ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. ì˜¤ì¡´ ìƒì„±: ê´‘í™”í•™ ë°˜ì‘ìœ¼ë¡œ ì˜¤ì¡´ì´ ëŠ˜ì–´ë‚˜ í˜¸í¡ê¸° ì¦ìƒì„ ì•…í™”ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë”°ë¼ì„œ ë°°ì¶œ ì €ê°ê³¼ ë…¸ì¶œì„ ì¤„ì´ëŠ” ë…¸ë ¥ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""".strip()
        
        self.prompt_tmpl = PromptTemplate.from_template(
    """{system_policy}

{fewshot_examples}

[User Question]
{user_question}
""".strip()
)
        
        
        self.retry_prompt_tmpl = PromptTemplate.from_template(
            """{system_policy}

ë„ˆì˜ ì§ì „ ì¶œë ¥ì´ ê·œì¹™/ìŠ¤íƒ€ì¼ì„ ì¶©ë¶„íˆ ì§€í‚¤ì§€ ëª»í–ˆë‹¤.
ì´ë²ˆì—ëŠ” ë°˜ë“œì‹œ ê·œì¹™ì„ ì§€í‚¤ê³  ë‹¤ì‹œ ë‹µí•˜ë¼.

{fewshot_examples}

[User Question]
{user_question}

[Your previous output]
{previous_output}
""".strip()
        )  
        #4.ì²´ì¸êµ¬ì„±
        self.chain =(
            self.prompt_tmpl
            | RunnableLambda(self._to_exaone_chatprompt)
            | RunnableLambda(self._run_hf)
            | StrOutputParser()
        ) 
        self.raw_chain = (
            self.prompt_tmpl
            | RunnableLambda(self._to_exaone_chatprompt)
            | RunnableLambda(self._run_hf)
            | StrOutputParser()
        )

        self.retry_chain = (
            self.retry_prompt_tmpl
            | RunnableLambda(self._to_exaone_chatprompt)
            | RunnableLambda(self._run_hf)
            | StrOutputParser()
        )

        #ë‚´ë¶€ ë©”ì„œë“œ : 1) LC prompt -> EXAONE chat template ë¬¸ìì—´
    def _to_exaone_chatprompt(self, prompt_value):  #ë­ì²´ì¸ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ì„ ì—‘ì‚¬ì› ë¬¸ìì—´ë¡œ ë°”ê¾¸ê¸°.  
        text = prompt_value.to_string() if hasattr(prompt_value, "to_string") else str(prompt_value)
        messages = [{"role": "user", "content": text}]
        chat_prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        return {"chat_prompt": chat_prompt}
        
        #ë‚´ë¶€ ë©”ì„œë“œ : 2). HF í˜¸ì¶œ + echo ì œê±°
    def _run_hf(self, d):
        chat_prompt = d["chat_prompt"]
        out = self.hf_llm.invoke(chat_prompt)

        if isinstance(out, str) and out.startswith(chat_prompt):   
            out = out[len(chat_prompt):]
        return out.strip()   
        
        #ì™¸ë¶€ ê³µê°œ ë©”ì„œë“œ: generate() 
    def generate(self, message: str) -> str:
        inputs = {
            "system_policy": self.system_policy,
            "fewshot_examples": self.fewshot_examples,
            "user_question": message,
        }
             # 1ì°¨ ì‹œë„
        try:
            return self.chain.invoke(inputs)
        except Exception:
            # previous_output í™•ë³´
            try:  
                previous_output = self.raw_chain.invoke(inputs)
            except Exception:
                previous_output = ""     

            retry_inputs = {
                "system_policy": self.system_policy,
                "fewshot_examples": self.fewshot_examples,
                "user_question": message,
                "previous_output": previous_output,
            }
                # ì¬ì‹œë„ 1íšŒ
            try:
                return self.retry_chain.invoke(retry_inputs)
            except Exception:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."  
                

  
        



